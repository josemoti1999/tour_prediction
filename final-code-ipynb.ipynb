{"cells":[{"metadata":{},"cell_type":"markdown","source":"## TRAIN.CSV\n* 7985 unique tour_ids and 1934 unique biker_ids\n* 13866 asked (3370 likes and 473 dislikes) - 3843 reviews - 10023 unanswered\n* 3843 reviews were given by 1934 bikers. All bikers liked atleast 1 tour. Only 99 out of 1934 bikers disliked atleast 1 tour. \n* 584 questions to invited bikers. This involves 290 bikers (they were invited for at least 1 tour). A biker invited for one tour may not be invited for another tour. Only 2 bikers were invited to all the tours they were asked questions on.\n* BG52448351 invited for 6 tours\n* CE44012189 invited for 6 tours\n* Some bikers were asked questions of same tour multiple times.\n* 27th April 2012 to 12th December 2012\n\n## BIKERS.CSV\n* Getting language, location, age as of now.\n\n## TOURS.CSV\n* \n\n## TEST.CSV\n* 2690 test queries - 1996 unique tour_ids, 297 unique bikers\n* 1358 unseen tours in test dataset. So better use features from tours.csv"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport datetime\nimport geopy\nfrom geopy.geocoders import Nominatim\nfrom sklearn.metrics.pairwise import cosine_distances, cosine_similarity\nfrom geopy.distance import geodesic\nimport sklearn\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, OrdinalEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgb\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, average_precision_score, accuracy_score\nfrom sklearn.model_selection import train_test_split, KFold\nfrom catboost import CatBoostClassifier, Pool, CatBoostRegressor\n#from bayes_opt import BayesianOptimization\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samp_sub = pd.read_csv('/kaggle/input/prml-data-contest-nov-2020/sample_submission.csv')\nsamp_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 200)\npd.set_option('display.max_columns', 200)\npd.set_option('display.width', 200)\npd.set_option('display.max_colwidth', -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/prml-data-contest-nov-2020/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['like'].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* age filled with average age.\n* member_since filled with dec 31\n* gender filled with male\n* timezone most frequent timezone\n* latitude and longitude filled of US most"},{"metadata":{"trusted":true},"cell_type":"code","source":"a = {'latitude': {'FR': 46.2276, 'US': 37.0902, 'ID': -0.7893, 'IR': 32.4279, 'GB': 55.3781, 'CN': 35.8617,\n              'LA': 19.8563, 'AR': -38.4161, 'ES': 40.4637, 'RU': 61.524, 'MY': 4.2105, 'KH': 12.5657,\n              'GE': 42.3154, 'IT': 41.8719, 'RO': 45.9432, 'PH': 12.8797, 'CA': 56.1304, 'KR': 35.9078,\n              'BR': -14.235, 'HK': 22.3964, 'HU': 47.1625, 'TR': 38.9637, 'TW':23.6978},\n 'longitude': {'FR': 2.2137, 'US': -95.7129, 'ID': 113.9213, 'IR': 53.688, 'GB': -3.436, 'CN': 104.1954,\n               'LA': 102.4955, 'AR': -63.6167, 'ES': -3.7492, 'RU': 105.3188, 'MY': 101.9758, 'KH': 104.991,\n               'GE': 43.3569, 'IT': 12.5674, 'RO': 24.9668, 'PH': 121.774, 'CA': -106.3468, 'KR': 127.7669,\n               'BR': -51.9253, 'HK': 114.1095, 'HU': 19.5033, 'TR': 35.2433, 'TW':120.9605}}\ncountry_df = pd.DataFrame(a)\ncountry_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/prml-data-contest-nov-2020/train.csv')\ntest_df = pd.read_csv('/kaggle/input/prml-data-contest-nov-2020/test.csv')\nbikers_df = pd.read_csv('/kaggle/input/prml-data-contest-nov-2020/bikers.csv')\ntours_df = pd.read_csv('/kaggle/input/prml-data-contest-nov-2020/tours.csv')\nbikers_network_df = pd.read_csv('/kaggle/input/prml-data-contest-nov-2020/bikers_network.csv')\ntour_convoy_df = pd.read_csv('/kaggle/input/prml-data-contest-nov-2020/tour_convoy.csv')\n\ndef conv_to_int(x):\n    try:\n        val = 2012-int(x)\n    except:\n        val = -1\n    return val\n    \n# preprocessing train and test\ntrain_df.drop_duplicates(subset=['biker_id','tour_id'],inplace=True, ignore_index=True)\ntest_df.drop_duplicates(subset=['biker_id','tour_id'],inplace=True, ignore_index=True)\n\n# preprocessing bikers_df\nbikers_df['member_since'].fillna('31-12-2012', inplace=True)\nbikers_df['member_since'].replace('--None', '31-12-2012', inplace=True)\nbikers_df['member_since'] = pd.to_datetime(bikers_df['member_since'], format='%d-%m-%Y')\nbikers_df['age'] = bikers_df['bornIn'].apply(lambda x:conv_to_int(x))\nbikers_df.loc[bikers_df['age']==-1,'age'] = round(bikers_df[bikers_df['age']!=-1]['age'].mean())\nbikers_df.set_index('biker_id', drop=True, inplace=True)\nbikers_df['gender'] = bikers_df['gender'].apply(lambda x:0 if x=='female' else 1)\nbikers_df['time_zone'].fillna(420, inplace=True)\nbikers_df.drop(labels=['bornIn','area'], axis=1, inplace=True)\nprint('Preprocessed bikers.csv')\n\n# preprocessing tours_df\ntours_df.set_index('tour_id', drop=True, inplace=True)\ngrouped_df = tours_df.groupby('biker_id')[['latitude','longitude']]\nlati_group_df = grouped_df.apply(lambda x:list(x['latitude'].unique())[0])\nlongi_group_df = grouped_df.apply(lambda x:list(x['longitude'].unique())[0])\nprint('Preprocessed tours.csv')\n\n# preprocessing bikers_network_df and tour_convoy_df\nbikers_network_df.set_index('biker_id', drop=True, inplace=True)\nbikers_network_df.fillna('', inplace=True)\nprint('Preprocessed bikers_network.csv')\ntour_convoy_df.set_index('tour_id', drop=True, inplace=True)\ntour_convoy_df.fillna('', inplace=True)\nprint('Preprocessed tour_convoy.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def latitude_func_(country_id):\n    if country_id=='PI':\n        country_id='PH'\n    return country_df.loc[country_id,'latitude']\n\ndef longitude_func_(country_id):\n    if country_id=='PI':\n        country_id='PH'\n    return country_df.loc[country_id,'longitude']\n\ndef full_preprocess(df_to, save_name):\n    # features from bikers_df\n    temp_df = df_to.set_index('biker_id', drop=True).join(bikers_df).reset_index()\n    temp_df['timestamp'] = pd.to_datetime(temp_df['timestamp'], format='%d-%m-%Y  %H:%M:%S')\n    temp_df['join_query_difference'] = (temp_df['timestamp']-temp_df['member_since']).dt.days\n    temp_df['month_joined'] = temp_df['member_since'].dt.month\n    temp_df['day_joined'] = temp_df['member_since'].dt.day\n    temp_df['month_query'] = temp_df['timestamp'].dt.month\n    temp_df['day_query'] = temp_df['timestamp'].dt.day\n    temp_df['hour_query'] = temp_df['timestamp'].dt.hour\n    temp_df['minute_query'] = temp_df['timestamp'].dt.minute\n    temp_df['biker_latitude'] = temp_df['location_id'].apply(latitude_func_)\n    temp_df['biker_longitude'] = temp_df['location_id'].apply(longitude_func_)\n\n    # features from tours_df\n    temp_df = temp_df.set_index('tour_id', drop=True).join(tours_df.loc[:,'biker_id'],rsuffix='_organizer').reset_index()\n    temp_df = temp_df.set_index('tour_id', drop=True).join(tours_df.loc[:,'city':'longitude'],rsuffix='_tour_').reset_index()\n    temp_df = temp_df.set_index('tour_id', drop=True).join(tours_df.loc[:,'w1':'w_other']).reset_index()\n    temp_df = temp_df.set_index('tour_id', drop=True).join(tours_df.loc[:,'tour_date']).reset_index()\n    temp_df.loc[temp_df['latitude'].isna(),'latitude'] = temp_df.loc[temp_df['latitude'].isna(),\n                                                                     'biker_id_organizer'].apply(lambda x:lati_group_df[x])\n    temp_df.loc[temp_df['longitude'].isna(),'longitude'] = temp_df.loc[temp_df['longitude'].isna(),\n                                                                     'biker_id_organizer'].apply(lambda x:longi_group_df[x])\n    set1 = set(temp_df.loc[temp_df['latitude'].isna(),'biker_id_organizer'].unique())\n    set2 = set(bikers_df.index.unique())\n    set3 = set(tours_df.loc[(tours_df['latitude'].isna())&(~tours_df['country'].isna()),:].index.unique())\n    set4 = set(temp_df.loc[temp_df['latitude'].isna(),'tour_id'].unique())\n    print(bikers_df.loc[set1.intersection(set2),'location_id'])\n    for b_id in set1.intersection(set2):\n        country_id = bikers_df.loc[b_id, 'location_id']\n        lati = latitude_func_(country_id)\n        longi = longitude_func_(country_id)\n        temp_df.loc[(temp_df['latitude'].isna())&(temp_df['biker_id_organizer']==b_id),['latitude','longitude']] = [lati, longi]\n    temp_df['latitude'] = temp_df['latitude'].fillna(latitude_func_('US'))\n    temp_df['longitude'] = temp_df['longitude'].fillna(longitude_func_('US'))\n    temp_df.drop(labels=['city','state','pincode','country'],axis=1, inplace=True)\n    temp_df['tour_date'] = pd.to_datetime(temp_df['tour_date'], format='%d-%m-%Y')\n    temp_df['join_tour_difference'] = (temp_df['tour_date']-temp_df['member_since']).dt.days\n    temp_df['year_tour'] = temp_df['tour_date'].dt.year\n    temp_df['month_tour'] = temp_df['tour_date'].dt.month\n    temp_df['day_tour'] = temp_df['tour_date'].dt.day\n    temp_df['tour_query_difference'] = (temp_df['timestamp']-temp_df['tour_date']).dt.days\n#     temp_df['biker_code_fl'] = temp_df['biker_id'].apply(lambda x:str(x)[0])\n#     temp_df['biker_code_sl'] = temp_df['biker_id'].apply(lambda x:str(x)[1])\n#     temp_df['biker_code'] = temp_df['biker_id'].apply(lambda x:str(x)[:2])\n#     temp_df['tour_code_fl'] = temp_df['tour_id'].apply(lambda x:str(x)[1])\n#     temp_df['tour_code_sl'] = temp_df['tour_id'].apply(lambda x:str(x)[1])\n#     temp_df['tour_code'] = temp_df['tour_id'].apply(lambda x:str(x)[:2])\n\n    # getting the order of tours for a given biker\n    num_queries = temp_df.groupby('biker_id').count()['tour_id'].to_frame()\n    num_queries.rename(columns={'tour_id':'num_queries_count'}, inplace=True)\n    temp_df = temp_df.join(num_queries, on='biker_id')\n    grouped = temp_df.groupby('biker_id')\n    concat_list = []\n    for i, df_sub in grouped:\n        df_sub = df_sub.sort_values(by=['timestamp','tour_date'], ignore_index=True)\n        df_sub['timestamp_index'] = df_sub.index/df_sub.index.max()\n        df_sub = df_sub.sort_values(by=['tour_date','timestamp'], ignore_index=True)\n        df_sub['tour_date_index'] = df_sub.index/df_sub.index.max()\n        concat_list.append(df_sub)\n    temp_df = pd.concat(concat_list, ignore_index=True)\n\n    # getting order of biker for a given tour\n    combined_df = pd.concat([train_df[['biker_id','tour_id','timestamp','like','dislike']],\n                             test_df[['biker_id','tour_id','timestamp']]],join='outer').reset_index(drop=True)\n    combined_df['like'] = combined_df['like'].fillna(0)\n    combined_df['dislike'] = combined_df['dislike'].fillna(0)\n    combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp'], format='%d-%m-%Y  %H:%M:%S')\n    combined_df = combined_df.join(bikers_df.loc[:,'member_since'], on='biker_id').reset_index(drop=True)\n    num_bikers_asked_total = combined_df.groupby('tour_id').count()['biker_id'].to_frame()\n    num_bikers_asked_total.rename(columns={'biker_id':'num_bikers_asked_total'}, inplace=True)\n    num_bikers_asked_train = train_df.groupby('tour_id').count()['biker_id'].to_frame()\n    num_bikers_asked_train.rename(columns={'biker_id':'num_bikers_asked_train'}, inplace=True)\n    num_bikers_like_dislike = combined_df.groupby('tour_id').sum()[['like','dislike']]\n    num_bikers_like_dislike.rename(columns={'like':'num_bikers_liked', 'dislike':'num_bikers_disliked'}, \n                                   inplace=True)\n\n    combined_df = combined_df.join(num_bikers_asked_total, on='tour_id')\n    combined_df = combined_df.join(num_bikers_asked_train, on='tour_id')\n    combined_df = combined_df.join(num_bikers_like_dislike, on='tour_id')\n    combined_df.fillna(0, inplace=True)\n    combined_df['percent_train_like'] = combined_df['num_bikers_liked']/combined_df['num_bikers_asked_train']\n    combined_df['percent_train_dislike'] = combined_df['num_bikers_disliked']/combined_df['num_bikers_asked_train']\n    combined_df.fillna(0, inplace=True)\n    grouped2 = combined_df.groupby('tour_id')\n    concat_list2 = []\n    for i, df_sub in grouped2:\n        df_sub = df_sub.sort_values(by=['timestamp','member_since'], ignore_index=True)\n        df_sub['biker_timestamp_index'] = (df_sub.index+1)/(df_sub.index.max()+1)\n        df_sub = df_sub.sort_values(by=['member_since','timestamp'], ignore_index=True)\n        df_sub['biker_member_since_index'] = (df_sub.index+1)/(df_sub.index.max()+1)\n        concat_list2.append(df_sub)\n    combined_df = pd.concat(concat_list2, ignore_index=True)\n    combined_df.drop(columns=['timestamp','like','dislike','member_since'], inplace=True)\n    assert(combined_df.isna().sum().sum()==0)\n    temp_df = pd.merge(temp_df, combined_df, how='left', on=['biker_id','tour_id'])\n\n    # from bikers_network_df and tour_convoy_df\n    temp_df = temp_df.set_index('biker_id', drop=True).join(bikers_network_df.loc[:,'friends']).reset_index()\n    temp_df = temp_df.set_index('tour_id', drop=True).join(tour_convoy_df.loc[:,:],rsuffix='_tour').reset_index()\n\n    temp_df['friends'] = temp_df['friends'].apply(lambda x:set(str(x).split()))\n    temp_df['going'] = temp_df['going'].apply(lambda x:set(str(x).split()))\n    temp_df['maybe'] = temp_df['maybe'].apply(lambda x:set(str(x).split()))\n    temp_df['invited_tour'] = temp_df['invited_tour'].apply(lambda x:set(str(x).split()))\n    temp_df['not_going'] = temp_df['not_going'].apply(lambda x:set(str(x).split()))\n\n    biker_loc_dict = bikers_df.loc[:,'location_id'].to_dict()\n\n    def mapper_biker_to_loc(x0, x1):\n        count = 0\n        for i in x0:\n            try:\n                if biker_loc_dict[i]==x1:\n                    count+=1\n            except:\n                pass\n        return count\n\n    temp_df['friends_loc'] = temp_df[['location_id','friends']].apply(lambda x:mapper_biker_to_loc(x[1],x[0]), axis=1)\n    temp_df['going_loc'] = temp_df[['location_id','going']].apply(lambda x:mapper_biker_to_loc(x[1],x[0]), axis=1)\n    temp_df['maybe_loc'] = temp_df[['location_id','maybe']].apply(lambda x:mapper_biker_to_loc(x[1],x[0]), axis=1)\n    temp_df['invited_tour_loc'] = temp_df[['location_id','invited_tour']].apply(lambda x:mapper_biker_to_loc(x[1],x[0]), axis=1)\n    temp_df['not_going_loc'] = temp_df[['location_id','not_going']].apply(lambda x:mapper_biker_to_loc(x[1],x[0]), axis=1)\n\n    temp_df['num_friends'] = temp_df['friends'].apply(lambda x:len(x))\n    temp_df['num_going'] = temp_df['going'].apply(lambda x:len(x))\n    temp_df['num_maybe'] = temp_df['maybe'].apply(lambda x:len(x))\n    temp_df['num_invited'] = temp_df['invited_tour'].apply(lambda x:len(x))\n    temp_df['num_not_going'] = temp_df['not_going'].apply(lambda x:len(x))\n\n    temp_df['friends_going'] = temp_df[['friends','going']].apply(lambda x:len(x[0].intersection(x[1])), axis=1)\n    temp_df['friends_maybe'] = temp_df[['friends','maybe']].apply(lambda x:len(x[0].intersection(x[1])), axis=1)\n    temp_df['friends_invited'] = temp_df[['friends','invited_tour']].apply(lambda x:len(x[0].intersection(x[1])), axis=1)\n    temp_df['friends_not_going'] = temp_df[['friends','not_going']].apply(lambda x:len(x[0].intersection(x[1])), axis=1)\n    temp_df['friend_organizer'] = temp_df[['friends','biker_id_organizer']].apply(lambda x:1 if str(x[0]) in x[1] else 0, axis=1)\n\n    temp_df['percent_going'] = temp_df['friends_going']/temp_df['num_friends']\n    temp_df['percent_maybe'] = temp_df['friends_maybe']/temp_df['num_friends']\n    temp_df['percent_invited'] = temp_df['friends_invited']/temp_df['num_friends']\n    temp_df['percent_not_going'] = temp_df['friends_not_going']/temp_df['num_friends']\n\n    temp_df['percent_going_2'] = temp_df['friends_going']/temp_df['num_going']\n    temp_df['percent_maybe_2'] = temp_df['friends_maybe']/temp_df['num_maybe']\n    temp_df['percent_invited_2'] = temp_df['friends_invited']/temp_df['num_invited']\n    temp_df['percent_not_going_2'] = temp_df['friends_not_going']/temp_df['num_not_going']\n\n    temp_df['percent_going_3'] = temp_df['going_loc']/temp_df['num_going']\n    temp_df['percent_maybe_3'] = temp_df['maybe_loc']/temp_df['num_maybe']\n    temp_df['percent_invited_3'] = temp_df['invited_tour_loc']/temp_df['num_invited']\n    temp_df['percent_not_going_3'] = temp_df['not_going_loc']/temp_df['num_not_going']\n    temp_df['percent_friends_3'] = temp_df['friends_loc']/temp_df['num_friends']\n\n    temp_df.fillna(0, inplace=True)\n\n    temp_df['is_organizer'] = (temp_df['biker_id']==temp_df['biker_id_organizer']).astype(int)\n    temp_df['geodesic_distance'] = temp_df.loc[:,['latitude',\n                                                  'longitude',\n                                                  'biker_latitude',\n                                                  'biker_longitude']].apply(lambda x:geodesic((x[0],\n                                                                                               x[1]),\n                                                                                              (x[2],\n                                                                                               x[3])).miles, axis=1)\n\n\n    temp_df = temp_df.drop(labels=['biker_id_organizer','friends','going','maybe','invited_tour','not_going'], axis=1)\n    temp_df.drop(labels=['timestamp','member_since','tour_date'], axis=1, inplace=True)\n    temp_df.to_csv('{}_data_cleaned_v10.csv'.format(save_name), index=False)\n    assert(temp_df.isna().sum().sum()==0)\n    assert(len(list(temp_df.columns))==len(set(temp_df.columns)))\n    return temp_df\ntrain_cleaned_df = full_preprocess(train_df, 'train')\ntrain_cleaned_df.sample(frac=1).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_cleaned_df = full_preprocess(test_df, 'test')\ntest_cleaned_df.sample(frac=1).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -r *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"EXP = 59\nNUM_FOLDS = 5\nCLASSIFIER_NAME = ['LGB']\nENC1 = 'ordinal'\nUSE_SCALER = True\nSEED = 101\nEXPO = 1\nNUM_SEEDS = 2\nNUM_LEAVES = 31\nNUM_BOOST_ROUNDS = 20000\nTRAIN_FULL = False\nVERBOSE = False\nDO_BALANCE = True\nPOSTPROCESS = True\nPOST_PROCESS2 = False\n\nMULTICLASS = False\nUSE_TOUR_ID = False\nBAYES_TUNING = False\nINIT_ROUNDS = 5\nOPT_ROUNDS = 15\nNUM_FEATURES_BASE = 160 + int(USE_TOUR_ID)\n\nif ENC1=='ordinal':\n    NUM_FEATURES = NUM_FEATURES_BASE\nif ENC1=='one_hot':\n    NUM_FEATURES = NUM_FEATURES_BASE + 38","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cleaned_df = pd.read_csv('../input/train-data-cleaned/train_data_cleaned_v10.csv')\ntest_cleaned_df = pd.read_csv('../input/train-data-cleaned/test_data_cleaned_v10.csv')\ndrop_cols = ['percent_train_like','percent_train_dislike',\n             'num_bikers_liked','num_bikers_disliked',\n             'biker_code','biker_code_fl','biker_code_sl',\n             'tour_code','tour_code_fl','tour_code_sl'\n            ]\ndrop_cols += []\ntrain_cleaned_df.drop(columns=drop_cols, inplace=True)\ntest_cleaned_df.drop(columns=drop_cols, inplace=True)\n\nfeatures = list(train_cleaned_df.copy().drop(labels=['like','dislike'], axis=1).columns)[2:]\ncate_features = ['language_id','location_id']\nif USE_TOUR_ID:\n    cate_features = ['tour_id']+cate_features\n    features = ['tour_id']+features\nif MULTICLASS:\n    train_cleaned_df['like'] = train_cleaned_df['like'] - train_cleaned_df['dislike'] +1\nif ENC1=='ordinal':\n    enc1 = OrdinalEncoder()\n    X_temp = train_cleaned_df.loc[:,cate_features].values\n    X_temp_test = test_cleaned_df.loc[:,cate_features].values\n    X_temp_combined = np.concatenate([X_temp, X_temp_test], axis=0)\n    enc1.fit(X_temp_combined)\n    X_temp_enc = enc1.transform(X_temp)\n    assert(X_temp_enc.shape[0]==(train_cleaned_df.shape[0]))\n    train_cleaned_df.loc[:,cate_features] = X_temp_enc\nelif ENC1=='one_hot':\n    u1 = train_cleaned_df['language_id'].nunique()\n    u2 = train_cleaned_df['location_id'].nunique()\n    enc1 = OneHotEncoder(drop='if_binary')\n    X_temp = train_cleaned_df.loc[:,['language_id','location_id']].values\n    enc1.fit(X_temp)\n    X_temp_enc = enc1.transform(X_temp).toarray()\n    encoded_features1 = list(enc1.get_feature_names(['language_id','location_id']))\n    assert(X_temp_enc.shape==(train_cleaned_df.shape[0],u1+u2))\n    train_cleaned_df.loc[:,encoded_features1] = X_temp_enc\n    train_cleaned_df.drop(labels=['language_id','location_id'], axis=1, inplace=True)\ncate_features_ind = [features.index(i) for i in cate_features]\n    \ndef get_data_preprocessed(train_cleaned_df, seed):\n    cross_val_df = train_cleaned_df.copy().groupby('biker_id')['like'].sum().to_frame()\n    cross_val_df.reset_index(inplace=True)\n    cross_val_df['fold'] = -1\n    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=seed)\n    for i, (_, val_index) in enumerate(kf.split(np.arange(cross_val_df.shape[0]))):\n        cross_val_df.loc[val_index,'fold'] = i\n    cross_val_df.set_index('biker_id', drop=True, inplace=True)\n    train_cleaned_df = train_cleaned_df.join(cross_val_df.loc[:,'fold'], on='biker_id')\n    train_cleaned_df['prediction_scores'] = -1\n    if DO_BALANCE:\n        subset = train_cleaned_df[train_cleaned_df['like']==1]\n        train_cleaned_df = pd.concat([train_cleaned_df, subset, subset],\n                                      ignore_index=True).sort_values(by=['tour_id','biker_id'], ignore_index=True)\n    return train_cleaned_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def map_calculate(temp_df, dislike):\n    \"\"\"\n    4 columns:biker_id, tour_id, like, prediction_scores\n    drop duplicates first - drop negatives in true_label\n    \"\"\"\n    temp_df = temp_df.drop_duplicates(ignore_index=True)\n    grouped = temp_df.groupby('biker_id')\n    map_score = 0; count = 0;\n    for i,df in grouped:\n        if dislike:\n            true = df.loc[:,'dislike'].values\n        else:\n            true = df.loc[:,'like'].values\n        if MULTICLASS:\n            true -= 1\n        true[true==-1] = 0\n        pred = df.loc[:,'prediction_scores'].values\n        if dislike:\n            ordered_label = df.sort_values(by='prediction_scores', ascending=False)['dislike'].values\n        else:\n            ordered_label = df.sort_values(by='prediction_scores', ascending=False)['like'].values\n        if true.sum()==0:\n            continue\n        ap_score = average_precision_score(true, pred)\n        map_score += ap_score\n        count+=1\n    return map_score/count\n\ndef do_cv_lgb(train_cleaned_df, param, clf_name):\n    clf_list = []\n    enc2_list = []\n    train_cleaned_df_train = train_cleaned_df.copy()\n    train_cleaned_df_train['prediction_scores_sum'] = 0\n    print(clf_name)\n    #assert(len(features)==NUM_FEATURES)\n\n    for i in range(NUM_FOLDS):   \n        #print('Fold-->{}'.format(i))\n        tour_sum = train_cleaned_df.loc[train_cleaned_df['fold']!=i,:].groupby('tour_id')['like','dislike'].sum()\n        tour_sum.reset_index(inplace=True)\n        tour_sum.columns = ['tour_id','like_sum','dislike_sum']\n        x_df_temp = train_cleaned_df.copy().drop(labels=['like','dislike'], axis=1)\n        X_train_df = x_df_temp.loc[x_df_temp['fold']!=i,:].drop(labels=['fold','prediction_scores'], axis=1)\n        Y_like_train = train_cleaned_df.loc[train_cleaned_df['fold']!=i,'like'].values\n        Y_dislike_train = train_cleaned_df.loc[train_cleaned_df['fold']!=i,'dislike'].values    \n        if TRAIN_FULL:\n            X_train_df = x_df_temp.loc[:,:].drop(labels=['fold','prediction_scores'], axis=1)\n            Y_like_train = train_cleaned_df.loc[:,'like'].values\n            Y_dislike_train = train_cleaned_df.loc[:,'dislike'].values    \n        Y_like_val = train_cleaned_df.loc[train_cleaned_df['fold']==i,'like'].values\n        Y_dislike_val = train_cleaned_df.loc[train_cleaned_df['fold']==i,'dislike'].values\n        X_val_df = x_df_temp.loc[x_df_temp['fold']==i,:].drop(labels=['fold','prediction_scores'], axis=1)\n        X_val_df.fillna(0, inplace=True)\n        if USE_TOUR_ID:\n            X_train = X_train_df.drop(labels=['biker_id'], axis=1).values\n            X_val = X_val_df.drop(labels=['biker_id'], axis=1).values\n        else:\n            X_train = X_train_df.drop(labels=['tour_id','biker_id'], axis=1).values\n            X_val = X_val_df.drop(labels=['tour_id','biker_id'], axis=1).values\n        if USE_SCALER:\n            enc2 = StandardScaler()\n            enc2.fit(X_train)\n            X_train_enc = enc2.transform(X_train)\n            X_val_enc = enc2.transform(X_val)\n            enc2_list.append(enc2)\n        else:\n            X_train_enc = X_train\n            X_val_enc = X_val\n        \n        if clf_name=='LGB':\n            train_data = lgb.Dataset(X_train_enc, label=Y_like_train)\n            val_data = lgb.Dataset(X_val_enc, label=Y_like_val)\n            bst = lgb.train(param, train_data, NUM_BOOST_ROUNDS, valid_sets=[val_data],\n                            feature_name=features, categorical_feature=cate_features,\n                            verbose_eval=VERBOSE, early_stopping_rounds=250)\n            Y_pred = bst.predict(X_val_enc, num_iteration=bst.best_iteration)\n            Y_pred_train = bst.predict(X_train_enc, num_iteration=bst.best_iteration)\n            pred_scores = Y_pred\n            pred_scores_train = Y_pred_train\n        elif clf_name=='CATBOOST':\n            train_data = Pool(data=X_train_enc, label=Y_like_train,cat_features=cate_features_ind)\n            val_data = Pool(data=X_val_enc, label=Y_like_val,cat_features=cate_features_ind)\n            bst = CatBoostClassifier(iterations=NUM_BOOST_ROUNDS,random_seed=SEED,\n                                     eval_metric='AUC')\n            bst.fit(train_data, eval_set=[val_data], use_best_model=True,\n                    verbose_eval=VERBOSE, early_stopping_rounds=250)\n            Y_pred = bst.predict_proba(X_val_enc)[:,1]\n            Y_pred_train = bst.predict_proba(X_train_enc)[:,1]\n            #Y_pred = bst.predict(X_val_enc)\n            #Y_pred_train = bst.predict(X_train_enc)\n            pred_scores = Y_pred\n            pred_scores_train = Y_pred_train\n        \n        if MULTICLASS:\n            pred_scores = Y_pred[:,2]-Y_pred[:,0]\n            pred_scores_train = Y_pred_train[:,2]-Y_pred_train[:,0]\n        train_cleaned_df.loc[train_cleaned_df['fold']==i,'prediction_scores'] = pred_scores\n        if not TRAIN_FULL:\n            train_cleaned_df_train.loc[train_cleaned_df['fold']!=i,'prediction_scores'] = pred_scores_train\n            train_cleaned_df_train.loc[train_cleaned_df['fold']!=i,'prediction_scores_sum'] += pred_scores_train\n        else:\n            train_cleaned_df_train.loc[:,'prediction_scores'] = pred_scores_train\n            train_cleaned_df_train.loc[:,'prediction_scores_sum'] += pred_scores_train\n        dislike = False\n        map_score = map_calculate(train_cleaned_df.loc[train_cleaned_df['fold']==i,\n                                                       ['biker_id','tour_id','like', 'dislike',\n                                                        'prediction_scores']], dislike)\n        map_score_train = map_calculate(train_cleaned_df_train.loc[train_cleaned_df['fold']!=i,\n                                                       ['biker_id','tour_id','like', 'dislike',\n                                                        'prediction_scores']], dislike)\n        print('Train-->{:.3f}, Val-->{:.3f}'.format(map_score_train, map_score))\n        clf_list.append(bst)\n        if TRAIN_FULL:\n            break\n    final_score = map_calculate(train_cleaned_df.loc[:,['biker_id','tour_id','like', 'prediction_scores']], dislike=False)\n    train_cleaned_df_train['prediction_scores'] = train_cleaned_df_train['prediction_scores_sum']\n    final_score_train = map_calculate(train_cleaned_df_train.loc[:,['biker_id','tour_id','like', 'prediction_scores']], dislike=False)\n    return train_cleaned_df, clf_list, enc2_list, final_score, final_score_train\n\ndef bayes_parameter_opt_lgb(train_cleaned_df,init_round, opt_round, random_seed):\n    \n    def lgb_eval(learning_rate, num_leaves, feature_fraction, bagging_fraction, \n                 max_depth, max_bin, min_data_in_leaf,min_sum_hessian_in_leaf,subsample):\n        param = {'metric':'auc'}\n        param['learning_rate'] = max(min(learning_rate, 1), 0)\n        param[\"num_leaves\"] = int(round(num_leaves))\n        param['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        param['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        param['max_depth'] = int(round(max_depth))\n        param['max_bin'] = int(round(max_bin))\n        param['min_data_in_leaf'] = int(round(min_data_in_leaf))\n        param['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n        param['subsample'] = max(min(subsample, 1), 0)\n        _, _, _, final_score = do_cv_lgb(train_cleaned_df, param)\n        return final_score\n    \n    def lgb_eval_2(num_leaves):\n        param = {'metric':'auc'}\n        param[\"num_leaves\"] = int(round(num_leaves))\n        _, _, _, final_score, final_score_train = do_cv_lgb(train_cleaned_df, param)\n        return final_score\n     \n    lgbBO = BayesianOptimization(lgb_eval_2, {'learning_rate': (0.001, 0.01),\n                                            'num_leaves': (30.5, 100.5),\n                                            #'feature_fraction': (0.1, 0.9),\n                                            #'bagging_fraction': (0.8, 1),\n                                            #'max_depth': (5, 30),\n                                            #'max_bin':(20,90),\n                                            #'min_data_in_leaf': (20, 80),\n                                            #'min_sum_hessian_in_leaf':(0,100),\n                                            #'subsample': (0.01, 1.0)\n                                           }, \n                                 random_state=random_seed)\n\n    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n    model_auc=[]\n    for model in range(len( lgbBO.res)):\n        model_auc.append(lgbBO.res[model]['target'])\n        \n    return (lgbBO.res[pd.Series(model_auc).idxmax()]['target'], \n            lgbBO.res[pd.Series(model_auc).idxmax()]['params'])\n\ndef lgb_tune_and_train(train_cleaned_df, seed, clf_name):\n    train_cleaned_df = get_data_preprocessed(train_cleaned_df, seed)\n    if BAYES_TUNING:\n        _, param = bayes_parameter_opt_lgb(train_cleaned_df,init_round=INIT_ROUNDS, \n                                           opt_round=OPT_ROUNDS, random_seed=seed)\n        param['learning_rate'] = max(min(param['learning_rate'], 1), 0)\n        param[\"num_leaves\"] = int(round(param['num_leaves']))\n        param['feature_fraction'] = max(min(param['feature_fraction'], 1), 0)\n        param['bagging_fraction'] = max(min(param['bagging_fraction'], 1), 0)\n        param['max_depth'] = int(round(param['max_depth']))\n        param['max_bin'] = int(round(param['max_bin']))\n        param['min_data_in_leaf'] = int(round(param['min_data_in_leaf']))\n        param['min_sum_hessian_in_leaf'] = param['min_sum_hessian_in_leaf']\n        param['subsample'] = max(min(param['subsample'], 1), 0)\n        param['metric']='auc'\n        #param['is_unbalance']=True\n        #param['boost_from_average']=False\n        train_cleaned_df, clf_list, enc2_list, final_score, final_score_train = do_cv_lgb(train_cleaned_df, param)\n    else:\n        param = {}\n        param['metric'] = 'auc'\n        param['num_leaves'] = NUM_LEAVES\n        if MULTICLASS:\n            param['objective'] = 'multiclass'\n            param['metric']='multi_logloss'\n            param['num_class']=3\n        train_cleaned_df, clf_list, enc2_list, final_score, final_score_train = do_cv_lgb(train_cleaned_df, param, clf_name)\n    print('Final Train-->{:.4f}, Val-->{:.4f}'.format(final_score_train, final_score))\n    return train_cleaned_df, clf_list, enc2_list, final_score\n\nclf_name_list = []\nfor s in range(NUM_SEEDS):\n    for s2, clf_name in enumerate(CLASSIFIER_NAME):\n        print(s)\n        clf_name_list.append(clf_name)\n        train_cleaned_df_sub, clf_sub, enc2_sub, final_score_sub = lgb_tune_and_train(train_cleaned_df,SEED+s, clf_name)\n        if s==0 and s2==0:\n            clf_list = clf_sub\n            enc2_list = enc2_sub\n            train_cleaned_df_final = train_cleaned_df_sub.copy().sort_values(by=['biker_id','tour_id'])\n            train_cleaned_df_final['prediction_scores'] = train_cleaned_df_final['prediction_scores']**EXPO\n        else:\n            clf_list += clf_sub\n            enc2_list += enc2_sub\n            train_cleaned_df_final['prediction_scores'] += train_cleaned_df_sub.sort_values(by=['biker_id',\n                                                                                                'tour_id'])['prediction_scores']**EXPO\n        final_score = map_calculate(train_cleaned_df_final.loc[:,['biker_id','tour_id',\n                                                            'like', 'prediction_scores']], dislike=False)\n        print('{:.4f}'.format(final_score))\ntrain_cleaned_df_final['prediction_scores'] /= NUM_SEEDS\ntrain_cleaned_df_final[['biker_id','tour_id','like','dislike',\n                  'prediction_scores']].to_csv('train_scores_exp{}-{:.3f}.csv'.format(EXP, final_score), index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(clf_list))\ntry:\n    print(len(clf_list2))\nexcept:\n    pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predicting test"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_cleaned_df = pd.read_csv('../input/train-data-cleaned/test_data_cleaned_v10.csv')\ntest_cleaned_df.drop(columns=drop_cols, inplace=True)\nX_temp = test_cleaned_df.loc[:,cate_features].values\n\nif ENC1=='ordinal':\n    print('Ordinal encoder')\n    X_temp_enc = enc1.transform(X_temp)\n    test_cleaned_df.loc[:,cate_features] = X_temp_enc\nelif ENC1=='one_hot':\n    print('One hot encoder')\n    X_temp_enc = enc1.transform(X_temp).toarray()\n    encoded_features1 = list(enc1.get_feature_names(['language_id','location_id']))\n    test_cleaned_df.loc[:,encoded_features1] = X_temp_enc\n    test_cleaned_df.drop(labels=['language_id','location_id'], axis=1, inplace=True)\n    \ntest_cleaned_df['prediction_scores'] = -1\nif USE_TOUR_ID:\n    X_test = test_cleaned_df.loc[:,:].drop(['biker_id','prediction_scores'],axis=1).values\nelse:\n    X_test = test_cleaned_df.loc[:,:].drop(['biker_id','prediction_scores','tour_id'],axis=1).values\n\nprint(len(clf_list))\nY_pred = np.zeros(shape=(test_cleaned_df.shape[0],))\nfor cc in range(len(clf_list)):\n    clf = clf_list[cc]\n    if USE_SCALER:\n        enc2 = enc2_list[cc]\n        X_test_enc = enc2.transform(X_test)\n    else:\n        X_test_enc = X_test\n    #assert(X_test_enc.shape==(test_cleaned_df.shape[0],NUM_FEATURES))\n    if clf_name_list[cc//5]=='LGB':\n        one_hot_predict = clf.predict(X_test_enc, num_iteration=clf.best_iteration)\n        if MULTICLASS:\n            Y_pred += one_hot_predict[:,2]-one_hot_predict[:,0]\n        else:\n            Y_pred += one_hot_predict\n        continue\n    elif clf_name_list[cc//5]=='CATBOOST':\n        X_test_enc = Pool(X_test_enc, cat_features=cate_features_ind)\n        one_hot_predict = clf.predict_proba(X_test_enc)[:,1]\n        if MULTICLASS:\n            Y_pred += one_hot_predict[:,2]-one_hot_predict[:,0]\n        else:\n            Y_pred += one_hot_predict\n        continue\nY_pred_final = Y_pred/len(clf_list)\ntest_cleaned_df['prediction_scores'] = Y_pred_final\nif ENC1=='ordinal':\n    inversed = enc1.inverse_transform(test_cleaned_df.loc[:,cate_features].values)\n    test_cleaned_df.loc[:,cate_features] = inversed\ntest_cleaned_df[['biker_id','tour_id',\n         'prediction_scores']].to_csv('test_scores_exp{}-{:.3f}.csv'.format(EXP, final_score), index=False)\ntest_cleaned_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def postprocessing_func(train_df, test_df):\n    train_df = train_df.drop_duplicates(subset=['biker_id','tour_id'],ignore_index=True)\n    test_df = test_df.drop_duplicates(subset=['biker_id','tour_id'],ignore_index=True)\n    pp_df_list = []\n    train_df = train_df.sort_values(by=['biker_id','tour_id'], ignore_index=True)\n    Y_like = train_df.like.values\n    fold_sorted = train_df.fold.values\n    for jj in range(NUM_FOLDS):\n        X_train = train_df.loc[train_df['fold']!=jj,:].reset_index(drop=True)\n        X_val = train_df.loc[train_df['fold']==jj,:].drop(['like','dislike',\n                                                           'fold'], axis=1).reset_index(drop=True)\n        pp_df_list.append(get_similarity_df(X_train, X_val))\n    pp_df = pd.concat(pp_df_list, ignore_index=True)\n    pp_df = pp_df.sort_values(by=['biker_id','tour_id'])\n    pp_df['like'] = Y_like\n    pp_df['fold'] = fold_sorted\n    pp_model_list = []\n    for kk in range(NUM_FOLDS):\n        X_train = pp_df.loc[pp_df['fold']!=kk,'prediction_scores_test':'same_tour']\n        X_val = pp_df.loc[pp_df['fold']==kk,'prediction_scores_test':'same_tour']\n        Y_like_train = pp_df.loc[pp_df['fold']!=kk,'like']\n        Y_like_val = pp_df.loc[pp_df['fold']==kk,'like']\n        train_data = lgb.Dataset(X_train, label=Y_like_train)\n        val_data = lgb.Dataset(X_val, label=Y_like_val)\n        param = {}\n        param['metric'] = 'auc'\n        param['objective'] = 'binary'\n        param['num_leaves'] = 8\n        bst = lgb.train(param, train_data, 10000, valid_sets=[val_data],\n                        verbose_eval=False, early_stopping_rounds=250)\n        pp_model_list.append(bst)\n        Y_pred = bst.predict(X_val, num_iteration=bst.best_iteration)\n        pp_df.loc[pp_df['fold']==kk,'prediction_scores'] = Y_pred\n        score = map_calculate(pp_df.loc[pp_df['fold']==kk,:], dislike=False)\n        print(score)\n    print('Final:',map_calculate(pp_df, dislike=False))\n    Y_pred = np.zeros(shape=(test_df.shape[0],))\n    for bst in pp_model_list:\n        pp_test_df = get_similarity_df(train_df,test_df)\n        X_test = pp_test_df.loc[:,'prediction_scores_test':'same_tour']\n        Y_pred += bst.predict(X_test, num_iteration=bst.best_iteration)\n    pp_test_df['prediction_scores'] = Y_pred/len(pp_model_list)\n    return pp_test_df\n\ndef get_similarity_df(train_df, test_df):\n    try:\n        X_temp = train_df.loc[:,cate_features].values\n        X_temp_enc = enc1.transform(X_temp)\n        train_df.loc[:,cate_features] = X_temp_enc\n    except:\n        pass\n    try:\n        X_temp = test_df.loc[:,cate_features].values\n        X_temp_enc = enc1.transform(X_temp)\n        test_df.loc[:,cate_features] = X_temp_enc\n    except:\n        pass\n    X = train_df.drop(['biker_id','tour_id','like','dislike','fold','prediction_scores'], axis=1)\n    Y = test_df.drop(['biker_id','tour_id','prediction_scores'], axis=1)\n    enc2_pp = StandardScaler()\n    enc2_pp.fit(X.values)\n    X_enc = enc2_pp.transform(X.values)\n    Y_enc = enc2_pp.transform(Y.values)\n    res = cosine_distances(X_enc, Y_enc)\n    res_arg_min = np.argmin(res, axis=0)\n    res_sim_df = train_df.loc[res_arg_min,['biker_id','tour_id','like',\n                                           'dislike','prediction_scores']+cate_features].reset_index(drop=True)\n    res_val_min = np.amin(res, axis=0)\n    res_diff_mean = np.mean(res, axis=0) - res_val_min\n    res_diff_max = np.amax(res, axis=0)-res_val_min\n    res_sorted = np.sort(res, axis=0)\n    assert(np.all(res_sorted[0,:]==res_val_min))\n    res_diff_sec = res_sorted[1,:]-res_sorted[0,:]\n    pp_df = test_df.copy()[['biker_id','tour_id', 'prediction_scores']+cate_features]\n    pp_df['res_diff_sec'] = res_diff_sec\n    pp_df['res_val_min'] = res_val_min\n    pp_df['res_diff_max'] = res_diff_max\n    pp_df['res_diff_mean'] = res_diff_max\n    pp_df = pp_df.join(res_sim_df, rsuffix='_train')\n    pp_df['same_location'] = pp_df['location_id']==pp_df['location_id_train']\n    pp_df['same_language'] = pp_df['language_id']==pp_df['language_id_train']\n    \n    #get num of common friends\n    #pp_df = pp_df.merge(bikers_network_df, on='biker_id', how='left')\n    #pp_df = pp_df.merge(bikers_network_df, left_on='biker_id_train', right_on='biker_id', how='left', suffixes=('','_train'))\n    #pp_df['friends'] = pp_df['friends'].apply(lambda x:set(str(x).split()))\n    #pp_df['friends_train'] = pp_df['friends_train'].apply(lambda x:set(str(x).split()))\n    #pp_df['friends_common_num'] = pp_df[['friends_train','friends']].apply(lambda x:len(x[0].intersection(x[1])), axis=1)\n    \n    pp_df['same_tour'] = pp_df['tour_id']==pp_df['tour_id_train']\n    pp_df = pp_df.drop(['tour_id_train','biker_id_train',\n                        'language_id_train','location_id_train',\n                        #'friends','friends_train'\n                       ]+cate_features, axis=1)\n    pp_df.rename(columns={'prediction_scores':'prediction_scores_test',\n                          'like':'like_sim','dislike':'dislike_sim'}, inplace=True)\n    return pp_df\nif POSTPROCESS:\n    bikers_network_df = pd.read_csv('/kaggle/input/prml-data-contest-nov-2020/bikers_network.csv')\n    bikers_network_df.fillna('', inplace=True)\n    pp_test_df = postprocessing_func(train_cleaned_df_final, test_cleaned_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def postprocessing_func2(train_cleaned_df_final, test_cleaned_df):\n    pp2_df = []\n    for i in range(NUM_FOLDS):\n        train_df = train_cleaned_df_final.drop_duplicates(subset=['biker_id','tour_id']).copy()\n        test_df = test_cleaned_df.copy()\n        train2_cols = features + ['biker_id', 'tour_id','prediction_scores']\n        X_train_df = train_df.loc[train_df['fold']!=i,train2_cols+['like','dislike']]\n        sub1 = X_train_df.groupby('tour_id').sum()[['like','dislike']]\n        sub2 = X_train_df.groupby('tour_id').mean()[['like','dislike']]\n        sub3 = X_train_df.groupby('tour_id').count()['biker_id'].to_frame()\n        sub3.columns = ['count']\n        sub = sub1.join(sub2,lsuffix='_sum',rsuffix='_mean')\n        sub = sub.join(sub3)\n        sub = sub[sub['count']>=4].reset_index()\n        X_train_df = X_train_df.merge(sub, on='tour_id')\n        X_train_df.dropna(inplace=True)\n        X_val_df = train_df.loc[train_df['fold']==i,train2_cols+['like','dislike']]\n        X_val_df = X_val_df.merge(sub, on='tour_id')\n        X_val_df.dropna(inplace=True)\n        X_val_df['new_preds'] = 0\n        features_pp = features+['prediction_scores','like_sum','dislike_sum','like_mean','dislike_mean','count']\n        train_data = Pool(data=X_train_df.loc[:,features_pp].values, \n                          label=X_train_df.loc[:,'like'].values, cat_features=cate_features_ind)\n        val_data = Pool(data=X_val_df.loc[:,features_pp].values, \n                        label=X_val_df.loc[:,'like'].values, cat_features=cate_features_ind)\n        bst = CatBoostClassifier(iterations=NUM_BOOST_ROUNDS,random_seed=SEED,eval_metric='AUC')\n        bst.fit(train_data, eval_set=[val_data], use_best_model=True,\n                verbose_eval=False, early_stopping_rounds=250)\n        Y_pred = bst.predict_proba(X_val_df.loc[:,features_pp].values)[:,1]\n        Y_pred_train = bst.predict_proba(X_train_df.loc[:,features_pp].values)[:,1]\n        pred_scores = Y_pred\n        pred_scores_train = Y_pred_train\n        X_val_df['new_preds'] = pred_scores\n        temp = X_val_df[['biker_id','tour_id','new_preds']].copy()\n        train_df = train_df.merge(temp,how='left', on=['biker_id','tour_id'])\n        train_df.fillna(0, inplace=True)\n        train_df['prediction_scores'] = np.where(train_df['new_preds']==0, \n                                                 train_df['prediction_scores'], \n                                                 train_df['new_preds'])\n        score = map_calculate(train_df.loc[train_df['fold']==i,\n                                           ['biker_id','tour_id','like', \n                                            'prediction_scores']], dislike=False)\n        print(score)\n        pp2_df.append(train_df.loc[train_df['fold']==i,:])\n    pp2_df = pd.concat(pp2_df)\n    return pp2_df\n\nif POST_PROCESS2:\n    pp2_df = postprocessing_func2(train_cleaned_df_final, test_cleaned_df)\n    pp2_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_sub_file(test_cleaned_df):\n    bikers_set = test_cleaned_df['biker_id'].unique()\n    bikers = []\n    tours = []\n    for biker in bikers_set:\n        subset = test_cleaned_df.loc[test_cleaned_df['biker_id']==biker,['biker_id','tour_id','prediction_scores']]\n        subset = subset.sort_values(by='prediction_scores', ascending=False).reset_index(drop=True)\n        tour_list = list(subset['tour_id'])\n        tour = \" \".join(tour_list)\n        bikers.append(biker)\n        tours.append(tour)\n    sample_submission =pd.DataFrame(columns=[\"biker_id\",\"tour_id\"])\n    sample_submission[\"biker_id\"] = bikers\n    sample_submission[\"tour_id\"] = tours\n    sample_submission.to_csv(\"sub{}_{:.3f}_5fold.csv\".format(EXP, final_score),index=False)\n    print(sample_submission.shape)\n    return sample_submission\n\nif POSTPROCESS:\n    print('Postprocessed')\n    sample_submission = get_sub_file(pp_test_df)\nelse:\n    sample_submission = get_sub_file(test_cleaned_df)\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if 'LGB' in CLASSIFIER_NAME:\n    importance = np.zeros(shape=(NUM_FEATURES,))\n    for clf in clf_list:\n        importance += clf.feature_importance()\n    importance/=len(clf_list)\n    features = list(test_cleaned_df.columns)[2:-1]\n    features = ['tour_id'] + features\n    importance_ordered = list(np.argsort(importance)[::-1])\n    features_ordered = [(features[i], importance[i]) for i in importance_ordered]\n    features_ordered","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if 'LGB' in CLASSIFIER_NAME:\n    plt.plot(importance)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_ordered","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 0.79097 - 0.744(val)"},{"metadata":{"trusted":true},"cell_type":"code","source":"EXP = 58\nNUM_FOLDS = 5\nCLASSIFIER_NAME = ['LGB']\nENC1 = 'ordinal'\nUSE_SCALER = False\nSEED = 101\nEXPO = 1\nNUM_SEEDS = 25\nNUM_LEAVES = 31\nNUM_BOOST_ROUNDS = 20000\nTRAIN_FULL = False\nVERBOSE = False\nDO_BALANCE = False\nPOSTPROCESS = False\nPOST_PROCESS2 = False\n\nMULTICLASS = False\nUSE_TOUR_ID = False\nBAYES_TUNING = False\nINIT_ROUNDS = 5\nOPT_ROUNDS = 15\nNUM_FEATURES_BASE = 160 + int(USE_TOUR_ID)\n\nif ENC1=='ordinal':\n    NUM_FEATURES = NUM_FEATURES_BASE\nif ENC1=='one_hot':\n    NUM_FEATURES = NUM_FEATURES_BASE + 38","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 0.79824"},{"metadata":{"trusted":true},"cell_type":"code","source":"EXP = 59\nNUM_FOLDS = 5\nCLASSIFIER_NAME = ['LGB']\nENC1 = 'ordinal'\nUSE_SCALER = True\nSEED = 101\nEXPO = 1\nNUM_SEEDS = 1\nNUM_LEAVES = 31\nNUM_BOOST_ROUNDS = 20000\nTRAIN_FULL = False\nVERBOSE = False\nDO_BALANCE = True\nPOSTPROCESS = True\nPOST_PROCESS2 = False\n\nMULTICLASS = False\nUSE_TOUR_ID = False\nBAYES_TUNING = False\nINIT_ROUNDS = 5\nOPT_ROUNDS = 15\nNUM_FEATURES_BASE = 160 + int(USE_TOUR_ID)\n\nif ENC1=='ordinal':\n    NUM_FEATURES = NUM_FEATURES_BASE\nif ENC1=='one_hot':\n    NUM_FEATURES = NUM_FEATURES_BASE + 38","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}